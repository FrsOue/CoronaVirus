{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import schedule\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import urllib\n",
    "import pypyodbc as podbc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'podbc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-56c7147b01dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# sql server cnx setup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m cnx = podbc.connect(\"Driver={SQL Server Native Client 11.0};\"\n\u001b[0m\u001b[0;32m      3\u001b[0m                                  \u001b[1;34m\"Server=10.0.0.1,1433;\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                  \u001b[1;34m\"Database=Covid19_global;\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                  \"Trusted_Connection=yes;\")\n",
      "\u001b[1;31mNameError\u001b[0m: name 'podbc' is not defined"
     ]
    }
   ],
   "source": [
    "# sql server cnx setup\n",
    "cnx = podbc.connect(\"Driver={SQL Server Native Client 11.0};\"\n",
    "                                 \"Server=10.0.0.1,1433;\"\n",
    "                                 \"Database=Covid19_global;\"\n",
    "                                 \"Trusted_Connection=yes;\")\n",
    "\n",
    "quoted = urllib.parse.quote_plus(\"Driver={SQL Server Native Client 11.0};\"\n",
    "                                 \"Server=myaddress;\"\n",
    "                                 \"Database=Covid19_global;\"\n",
    "                                 \"Trusted_Connection=yes;\")\n",
    "\n",
    "engine = create_engine('mssql+pyodbc:///?odbc_connect={}'.format(quoted))\n",
    "\n",
    "\n",
    "\n",
    "def data_ETL():\n",
    "    # Data collection\n",
    "    data_confirmed = pd.read_csv(\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\")\n",
    "    data_deaths = pd.read_csv(\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv\")\n",
    "    data_recovered = pd.read_csv(\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv\")\n",
    "    data_confirmed_US = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv')\n",
    "    data_deaths_US = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv')\n",
    "    \n",
    "    \n",
    "    # cration Dates list\n",
    "    column_dates = data_confirmed_US.columns\n",
    "    column_dates = column_dates.drop(labels =['UID', 'iso2', 'iso3', 'code3', 'FIPS', 'Admin2', 'Province_State',\n",
    "                                          'Country_Region', 'Lat', 'Long_', 'Combined_Key'])\n",
    "\n",
    "    # supprimer les colone inutile dans US confirmed et deaths df\n",
    "    data_confirmed_US.drop(['UID', 'iso2', 'iso3', 'code3', 'FIPS', 'Admin2', 'Combined_Key'], axis = 1, inplace=True)\n",
    "    data_deaths_US.drop(['UID', 'iso2', 'iso3', 'code3', 'FIPS', 'Admin2', 'Combined_Key', 'Population'], axis = 1, inplace=True)\n",
    "    \n",
    "    # calcul de la moyenne lat et log\n",
    "    lat_long_avg = data_confirmed_US.groupby(['Province_State', 'Country_Region'], as_index=False).agg({'Lat':'median', 'Long_':'median'})\n",
    "\n",
    "    # somme des valeurs \n",
    "    data_confirmed_US = data_confirmed_US.groupby(['Province_State', 'Country_Region'], as_index = False)[column_dates].sum()\n",
    "    data_deaths_US = data_deaths_US.groupby(['Province_State', 'Country_Region'], as_index = False)[column_dates].sum()\n",
    "    \n",
    "    # Left join avg lat et long\n",
    "    data_confirmed_US = data_confirmed_US.merge(lat_long_avg, how = 'left',\n",
    "                                               left_on = ['Province_State', 'Country_Region'],\n",
    "                                               right_on = ['Province_State', 'Country_Region'])\n",
    "    data_deaths_US = data_deaths_US.merge(lat_long_avg, how = 'left',\n",
    "                                               left_on = ['Province_State', 'Country_Region'],\n",
    "                                               right_on = ['Province_State', 'Country_Region'])\n",
    "    # raname colone\n",
    "    data_confirmed_US = data_confirmed_US.rename(columns = {'Province_State':'Province/State', 'Country_Region': 'Country/Region', 'Long_':'Long'})\n",
    "    data_deaths_US = data_deaths_US.rename(columns = {'Province_State':'Province/State', 'Country_Region': 'Country/Region', 'Long_':'Long'})\n",
    "\n",
    "    # column order\n",
    "    data_confirmed_US = data_confirmed_US[data_confirmed.columns]\n",
    "    data_deaths_US = data_deaths_US[data_confirmed.columns]\n",
    "\n",
    "    # suprimer ligne US de global conformed et deaths, en veut ajouter les donnée de US par sate et non pas la somme tatal \n",
    "    data_confirmed = data_confirmed[data_confirmed['Country/Region'] != 'US']\n",
    "    data_deaths = data_deaths[data_deaths['Country/Region'] != 'US']\n",
    "\n",
    "    #ajouter les donnée US au donnée global\n",
    "    data_confirmed = pd.concat([data_confirmed, data_confirmed_US], ignore_index= True)\n",
    "    data_deaths = pd.concat([data_deaths, data_deaths_US], ignore_index= True)\n",
    "    \n",
    "    # un-Pivotin gthe data\n",
    "    # colone valeur contenant les valeur des cas de chaque jours(cumulé)\n",
    "\n",
    "    data_confirmed2 = pd.melt(data_confirmed, id_vars=['Province/State', 'Country/Region', 'Lat', 'Long'], var_name=['Date'])\n",
    "    data_deaths2 =  pd.melt(data_deaths, id_vars=['Province/State', 'Country/Region', 'Lat', 'Long'], var_name=['Date'])\n",
    "    data_recovered2 =  pd.melt(data_recovered, id_vars=['Province/State', 'Country/Region', 'Lat', 'Long'], var_name=['Date'])\n",
    "    \n",
    "    # Convertir la nouvelle colone Date to date type\n",
    "    data_confirmed2.Date = pd.to_datetime(data_confirmed2.Date)\n",
    "    data_deaths2.Date = pd.to_datetime(data_deaths2.Date)\n",
    "    data_recovered2.Date = pd.to_datetime(data_recovered2.Date)\n",
    "    \n",
    "    # Renomer les colone value \n",
    "    data_confirmed2.columns = data_confirmed2.columns.str.replace(\"value\", \"Confirmed\")\n",
    "    data_deaths2.columns = data_deaths2.columns.str.replace('value', \"Deaths\")\n",
    "    data_recovered2.columns = data_recovered2.columns.str.replace(\"value\", 'Recovered')\n",
    "    \n",
    "    # gérer les valeurs manquantes de la variable \"Province/State\" : fill them with Coubtry/regiin values\n",
    "    data_confirmed2['Province/State'].fillna(np.nan, inplace = True)\n",
    "    data_deaths2['Province/State'].fillna(np.nan, inplace = True)\n",
    "    data_recovered2['Province/State'].fillna(np.nan, inplace = True)\n",
    "\n",
    "    data_confirmed2['Province/State'].fillna(data_confirmed2['Country/Region'], inplace = True)\n",
    "    data_deaths2['Province/State'].fillna(data_deaths2['Country/Region'], inplace =True)\n",
    "    data_recovered2['Province/State'].fillna(data_recovered2['Country/Region'], inplace = True)\n",
    "    \n",
    "    # confirmer + deaths + recovery\n",
    "    full_data = data_confirmed2.merge(data_deaths2[['Province/State', 'Country/Region', 'Date', 'Deaths']],\n",
    "                                     how= 'left',\n",
    "                                     left_on = ['Province/State', 'Country/Region', 'Date'],\n",
    "                                     right_on= ['Province/State', 'Country/Region', 'Date']\n",
    "                                     )\n",
    "\n",
    "    full_data = full_data.merge(data_recovered2[['Province/State', 'Country/Region', 'Date', 'Recovered']],\n",
    "                                how= 'left',\n",
    "                                left_on = ['Province/State', 'Country/Region', 'Date'],\n",
    "                                right_on= ['Province/State', 'Country/Region', 'Date'])\n",
    "    \n",
    "    full_data[\"Recovered\"].fillna(np.nan, inplace = True)\n",
    "    full_data[\"Recovered\"].fillna(0, inplace = True)\n",
    "    full_data['Recovered'] = full_data['Recovered'].astype('int64')\n",
    "    \n",
    "    # Ajouter une nouvelle colone contenant l'année et le mois : sera utile dans la parite viz\n",
    "    #full_data['Month-Year'] = full_data['Date'].dt.strftime('%b-%Y')\n",
    "    \n",
    "    # créer des colone contenant daily confirmed, daily death and daily recovered cases \n",
    "    # new df\n",
    "    full_data2 = full_data.copy()\n",
    "\n",
    "    #new colone date - 1 \n",
    "    full_data2['Date-1'] = full_data2['Date'] + pd.Timedelta(days = 1)\n",
    "    full_data2.rename(columns = {'Confirmed': 'Confirmed - 1', 'Deaths':'Deaths - 1', 'Recovered': 'Recovered - 1',\n",
    "                                'Date': 'Date Minus 1'}, inplace = True)\n",
    "    \n",
    "    #join mydata with the df created\n",
    "    full_data3 = full_data.merge(full_data2[['Province/State', 'Country/Region', 'Confirmed - 1', 'Deaths - 1', 'Recovered - 1',\n",
    "                                             'Date-1', 'Date Minus 1']], \n",
    "                                 how= \"left\",\n",
    "                                 left_on= ['Province/State', 'Country/Region', 'Date'],\n",
    "                                 right_on= ['Province/State', 'Country/Region', 'Date-1'])\n",
    "    \n",
    "    # calcule des cas par jour\n",
    "    full_data3['Confirmed Daily'] = full_data3['Confirmed'] - full_data3['Confirmed - 1']\n",
    "    full_data3['Deaths Daily'] = full_data3[\"Deaths\"] - full_data3['Deaths - 1']\n",
    "    full_data3['Recovered Daily'] = full_data3[\"Recovered\"] - full_data3['Recovered - 1']\n",
    "    \n",
    "    # ajouter manuellment les valeurs pour le premier jours de notre jeux de donnée\n",
    "    full_data3['Confirmed Daily'].loc[full_data3['Date'] == '2020-01-22'] = full_data3['Confirmed']\n",
    "    full_data3['Deaths Daily'].loc[full_data3['Date'] == '2020-01-22'] = full_data3['Deaths']\n",
    "    full_data3['Recovered Daily'].loc[full_data3['Date'] == '2020-01-22'] = full_data3['Recovered']\n",
    "\n",
    "    full_data3 = full_data3.drop(['Confirmed - 1', 'Deaths - 1',\n",
    "           'Recovered - 1', 'Date-1', 'Date Minus 1'], axis= 1 )\n",
    "    \n",
    "    # remplacer les val nan dans recovery avec 0\n",
    "    full_data3[\"Recovered Daily\"].fillna(np.nan, inplace = True)\n",
    "    full_data3[\"Recovered Daily\"].fillna(0, inplace = True)\n",
    "    \n",
    "    # change variable de type float au type int \n",
    "    full_data3['Confirmed Daily'] = full_data3['Confirmed Daily'].astype('int64')\n",
    "    full_data3['Deaths Daily'] = full_data3['Deaths Daily'].astype('int64')\n",
    "    full_data3['Recovered Daily'] = full_data3['Recovered Daily'].astype('int64')\n",
    "    full_data3['Recovered'] = full_data3['Recovered'].astype('int64')\n",
    "    \n",
    "    # Moratlaity rate & recovery rate\n",
    "    for ind, row in full_data3.iterrows():\n",
    "        full_data3.loc[ind, 'Recovery Rate'] = round(row[\"Recovered Daily\"] / row['Confirmed Daily'] if row['Confirmed Daily'] != 0 else 0, 3)\n",
    "        full_data3.loc[ind, 'Mortality Rate'] = round(row[\"Deaths Daily\"] / row['Confirmed Daily'] if row['Confirmed Daily'] != 0 else 0, 3)\n",
    "\n",
    "\n",
    "    # Exporting teh data to csv\n",
    "    #full_data3.to_csv('Corono.csv', sep ='\\t', index = False)\n",
    "    \n",
    "    # export data to sql\n",
    "    full_data3.to_sql('Daily_Covid_Data', schema='dbo', con = engine, if_exists = 'replace')\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule.every().day.at('00:01').do(data_ETL)\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(999)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
